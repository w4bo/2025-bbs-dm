{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsElqAaj4Sse"
      },
      "source": [
        "# Install Spark (already done in the VMs)\n",
        "\n",
        "Run the following code to install Spark in your local environment (this must be done only once).\n",
        "\n",
        "1. **IMPORTANT**: Java JDK (version 8, 12 or 17; **12** is recommended) and Python **3.11** (Spark 3.5.1 has compatibility issues with above versions of Python) must be installed in your machine.\n",
        "1. Download Spark at [this link](https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz).\n",
        "1. Run the next cell by changing the path to where you have downloaded the file above (alternatively, just unpack the folder into C:\\)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbvEUbWIHm2s"
      },
      "outputs": [],
      "source": [
        "!tar xf \"C:\\\\Users\\\\YOUR_USERNAME\\\\Downloads\\\\spark-3.5.1-bin-hadoop3.tgz\" -C \"C:\\\\\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Run the next command to install Spark's library for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3.11 -m pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Run the following code to set up the environment variables (uncomment only those that you need to set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the comments below to add the paths to your Java and Spark homes\n",
        "\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-12.0.2\" # Change this to the path of your JDK folder\n",
        "# os.environ[\"SPARK_HOME\"] = \"C:\\\\spark-3.5.1-bin-hadoop3\" # Change this to the directory where you have extracted the .tgz file by running the command in the above cell\n",
        "# os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
        "# os.environ[\"PATH\"]=os.environ[\"PATH\"]+\";\"+os.environ[\"SPARK_HOME\"]+\"\\\\bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **If you run on Windows**, you also need to download the following files and put them in the folder \"spark-3.5.1-bin-hadoop3/bin\" (see point 3):\n",
        "    - The Winutils.exe file at [this link](https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/winutils.exe)\n",
        "    - The hadoop.dll file at [this link](https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/hadoop.dll)\n",
        "        - Notice that some browsers block this download because .dll files could be dangerous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize Spark\n",
        "\n",
        "This must be done every time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4oTFM5YtJvv7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:/spark-3.5.1-bin-hadoop3'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find() # Should return the directory of the Spark home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KJlzVAmbJ9vL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://localhost:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=Colab>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "os.environ[\"SPARK_LOCAL_HOSTNAME\"] = \"localhost\"\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Local Spark\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBd7XwkFBDEF"
      },
      "source": [
        "# Spark: working with RDDs\n",
        "\n",
        "Check the documentation: [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcoFwGvm4gj6"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto",
        "id": "vlFswJyWytKG"
      },
      "outputs": [],
      "source": [
        "# let's create a simple example\n",
        "riddle1 = \"over the bench the sheep lives under the bench the sheep dies\"\n",
        "riddle2 = [\"over the bench the sheep lives\", \"under the bench the sheep dies\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto",
        "id": "WHywj4BmytKH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n"
          ]
        }
      ],
      "source": [
        "# create an RDD from the `riddle` string\n",
        "rdd1 = sc.parallelize(riddle1.split(\" \"))\n",
        "# each tuple of the RDD corresponds to a single word\n",
        "\n",
        "print(rdd1)\n",
        "# why is there no result returned?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto",
        "id": "3oYjBLnOytKI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['over', 'the', 'bench', 'the', 'sheep', 'lives', 'under', 'the', 'bench', 'the', 'sheep', 'dies']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compute the RDD\n",
        "print(rdd1.collect())\n",
        "rdd1.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "exzQLruZ9qgg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['over the bench the sheep lives', 'under the bench the sheep dies']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd2 = sc.parallelize(riddle2)\n",
        "print(rdd2.collect())\n",
        "rdd2.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyH7halU9HiB"
      },
      "source": [
        "## Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sndBHyEF86T5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['OVER',\n",
              " 'THE',\n",
              " 'BENCH',\n",
              " 'THE',\n",
              " 'SHEEP',\n",
              " 'LIVES',\n",
              " 'UNDER',\n",
              " 'THE',\n",
              " 'BENCH',\n",
              " 'THE',\n",
              " 'SHEEP',\n",
              " 'DIES']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# map: returns a new RDD by applying a function to each of the elements in the original RDD\n",
        "rdd1.map(lambda whatever: whatever.upper()).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9MlPRBd_-Cl1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['over', 'the', 'bench', 'the', 'sheep', 'lives'],\n",
              " ['under', 'the', 'bench', 'the', 'sheep', 'dies']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# flatMap: returns a new RDD by applying the function to every element of the parent RDD and then flattening the result\n",
        "rdd2.flatMap(lambda s: s.split(\" \")).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UlT_jxmH9Myx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['under']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filter: returns a new RDD containing only the elements in the parent RDD that satisfy the function inside filter\n",
        "rdd1.filter(lambda s: s.startswith(\"u\")).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QxxJdRxW-Xcj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['over', 'the', 'bench', 'sheep', 'lives', 'under', 'dies']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# distinct: returns a new RDD that contains only the distinct elements in the parent RDD\n",
        "rdd1.distinct().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dBAh2Gs8-fdM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('over', [1]),\n",
              " ('the', [1, 1, 1, 1]),\n",
              " ('bench', [1, 1]),\n",
              " ('sheep', [1, 1]),\n",
              " ('lives', [1]),\n",
              " ('under', [1]),\n",
              " ('dies', [1])]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# groupByKey: groups the values for each key in the (key, value) pairs of the RDD into a single sequence\n",
        "rdd1.map(lambda s: (s,1)).groupByKey().mapValues(list).collect()\n",
        "\n",
        "# (first map converts to a key-value RDD)\n",
        "# (mapValues is a map that operates only on the values - in this case, used to convert from ResultIterable to List for printing reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cH1dxiGl_cS5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('over', (1, 2)),\n",
              " ('the', (4, 0.25)),\n",
              " ('bench', (2, 1.0)),\n",
              " ('sheep', (2, 1.0)),\n",
              " ('lives', (1, 2)),\n",
              " ('under', (1, 2)),\n",
              " ('dies', (1, 2))]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reduceByKey: when called on a key-value RDD, returns a new dataset in which the values for each of its key are aggregated\n",
        "rdd1.map(lambda s: (s,(1,2))).reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1]/v2[1])).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-XJWbbv6_0Tw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('bench', 1),\n",
              " ('bench', 1),\n",
              " ('dies', 1),\n",
              " ('lives', 1),\n",
              " ('over', 1),\n",
              " ('sheep', 1),\n",
              " ('sheep', 1),\n",
              " ('the', 1),\n",
              " ('the', 1),\n",
              " ('the', 1),\n",
              " ('the', 1),\n",
              " ('under', 1)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sortByKey: returns a new RDD with (key,value) pairs of parent RDD in sorted order according to the key\n",
        "rdd1.map(lambda s: (s,1)).sortByKey().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SGIn90U0Md8j"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('BO', 'Bologna', 'Emilia-Romagna'),\n",
              " ('RA', 'Ravenna', 'Emilia-Romagna'),\n",
              " ('MI', 'Milan', 'Lombardia')]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# join: starting from two RDD with (key, value1) and (key, value2) pairs, returns a new RDD with (key, (value1, value2)) pairs\n",
        "\n",
        "# rddProvinces: (initials, name, region)\n",
        "rddProvinces = sc.parallelize([(\"BO\", \"Bologna\", \"Emilia-Romagna\"),(\"RA\", \"Ravenna\", \"Emilia-Romagna\"),(\"MI\", \"Milan\", \"Lombardia\")])\n",
        "# rddPeople: (id, name, province)\n",
        "rddPeople = sc.parallelize([(1, \"Enrico\", \"RA\"),(2, \"Alice\", \"RA\"),(3, \"Bob\", \"BO\"),(4, \"Charlie\", \"FC\")])\n",
        "\n",
        "# This does not work\n",
        "rddPeople.join(rddProvinces).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gQ54N0nKjmJC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('BO', ('Bologna', 'Emilia-Romagna')),\n",
              " ('RA', ('Ravenna', 'Emilia-Romagna')),\n",
              " ('MI', ('Milan', 'Lombardia'))]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rddProvinces2 = rddProvinces.map(lambda p: (p[0], (p[1],p[2])))\n",
        "rddProvinces2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PmG1337hk0a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('RA', (1, 'Enrico')),\n",
              " ('RA', (2, 'Alice')),\n",
              " ('BO', (3, 'Bob')),\n",
              " ('FC', (4, 'Charlie'))]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rddPeople2 = rddPeople.map(lambda p: (p[2], (p[0],p[1])))\n",
        "rddPeople2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AJaZHHXYj-vp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('BO', ((3, 'Bob'), ('Bologna', 'Emilia-Romagna'))),\n",
              " ('RA', ((1, 'Enrico'), ('Ravenna', 'Emilia-Romagna'))),\n",
              " ('RA', ((2, 'Alice'), ('Ravenna', 'Emilia-Romagna')))]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rddPeople2.join(rddProvinces2).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKJ_7MCsAIy_"
      },
      "source": [
        "## Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XQXX-d20_vWo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['over',\n",
              " 'the',\n",
              " 'bench',\n",
              " 'the',\n",
              " 'sheep',\n",
              " 'lives',\n",
              " 'under',\n",
              " 'the',\n",
              " 'bench',\n",
              " 'the',\n",
              " 'sheep',\n",
              " 'dies']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# collect: returns a list that contains all the elements of the RDD\n",
        "rdd1.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FXuL9K2qATfL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# count: returns the number of elements in the RDD\n",
        "rdd1.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YI10eDDnAWe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reduce: aggregates the elements of the RDD using a function that takes two elements of the RDD as input and gives the result\n",
        "sc.parallelize([1, 2, 3, 4, 5]).reduce( lambda x, y: x * y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CgN5qtwBAvIv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['over', 'the', 'bench', 'the', 'sheep']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take: returns the first n elements of RDD in the same order\n",
        "rdd1.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yS8SuYRQvV6F"
      },
      "outputs": [],
      "source": [
        "# saveAsTextFile: saves the content of the RDD to a file\n",
        "rdd1.saveAsTextFile(\"rdd1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmosJNIOA5S_"
      },
      "source": [
        "## Comprehension exercise\n",
        "\n",
        "- Work in pairs. \n",
        "- Take a look at the following jobs, understand the meaning of each transformation and the logic of each job.\n",
        "- Prepare to discuss them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto",
        "id": "mGRkO7_oytKK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['U', 'N', 'D', 'E', 'R']]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1\\\n",
        "    .map(lambda s: s.upper())\\\n",
        "    .filter(lambda s: s.startswith(\"U\"))\\\n",
        "    .map(lambda s: list(s))\\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto",
        "id": "Wrr37cZ2ytKL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 4),\n",
              " ('bench', 2),\n",
              " ('sheep', 2),\n",
              " ('over', 1),\n",
              " ('lives', 1),\n",
              " ('under', 1),\n",
              " ('dies', 1)]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1\\\n",
        "    .map(lambda s: (s, 1))\\\n",
        "    .reduceByKey(lambda a, b: a + b)\\\n",
        "    .sortBy(lambda x: x[1], False)\\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zxBoMsnTCATh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('b', 5.0),\n",
              " ('s', 5.0),\n",
              " ('l', 5.0),\n",
              " ('u', 5.0),\n",
              " ('o', 4.0),\n",
              " ('d', 4.0),\n",
              " ('t', 3.0)]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1\\\n",
        "  .map(lambda s: (s[0], (len(s),1)))\\\n",
        "  .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
        "  .mapValues(lambda x: x[0]/x[1])\\\n",
        "  .sortBy(lambda x: x[1], False)\\\n",
        "  .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXrRXAAEtet"
      },
      "source": [
        "# Spark: working with DataFrames\n",
        "\n",
        "Check the documentation: [here](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.DataFrame.html).\n",
        "\n",
        "What is different from Pandas' DataFrames?\n",
        "\n",
        "- Spark supports parallelization (Pandas doesn't), thus it's more suitable for big data processing\n",
        "- Spark follows Lazy Execution, which means that a task is not executed until an action is performed (Pandas follows Eager Execution, which means task is executed immediately)\n",
        "- Spark has immutability (Pandas has mutability)\n",
        "- The data structure is similar, the APIs are different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IGChYOQWRZku"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
            "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(\"../datasets/housing.csv\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_aSykf1wV8dB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      longitude latitude housing_median_age total_rooms total_bedrooms  \\\n",
            "0       -122.23    37.88               41.0       880.0          129.0   \n",
            "1       -122.22    37.86               21.0      7099.0         1106.0   \n",
            "2       -122.24    37.85               52.0      1467.0          190.0   \n",
            "3       -122.25    37.85               52.0      1274.0          235.0   \n",
            "4       -122.25    37.85               52.0      1627.0          280.0   \n",
            "...         ...      ...                ...         ...            ...   \n",
            "20635   -121.09    39.48               25.0      1665.0          374.0   \n",
            "20636   -121.21    39.49               18.0       697.0          150.0   \n",
            "20637   -121.22    39.43               17.0      2254.0          485.0   \n",
            "20638   -121.32    39.43               18.0      1860.0          409.0   \n",
            "20639   -121.24    39.37               16.0      2785.0          616.0   \n",
            "\n",
            "      population households median_income median_house_value ocean_proximity  \n",
            "0          322.0      126.0        8.3252           452600.0        NEAR BAY  \n",
            "1         2401.0     1138.0        8.3014           358500.0        NEAR BAY  \n",
            "2          496.0      177.0        7.2574           352100.0        NEAR BAY  \n",
            "3          558.0      219.0        5.6431           341300.0        NEAR BAY  \n",
            "4          565.0      259.0        3.8462           342200.0        NEAR BAY  \n",
            "...          ...        ...           ...                ...             ...  \n",
            "20635      845.0      330.0        1.5603            78100.0          INLAND  \n",
            "20636      356.0      114.0        2.5568            77100.0          INLAND  \n",
            "20637     1007.0      433.0           1.7            92300.0          INLAND  \n",
            "20638      741.0      349.0        1.8672            84700.0          INLAND  \n",
            "20639     1387.0      530.0        2.3886            89400.0          INLAND  \n",
            "\n",
            "[20640 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "# Switching from Spark to Pandas\n",
        "pandasDF = df.toPandas()\n",
        "print(pandasDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cFUtMUaUWTY5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
            "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Switching from Pandas to Spark\n",
        "df = spark.createDataFrame(pandasDF)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFAs2zBHPLTA"
      },
      "outputs": [],
      "source": [
        "# select: returns a new DataFrame with only selected columns (similar to a map on RDDs)\n",
        "df.select('population','median_house_value').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmClDtFaQqO7"
      },
      "outputs": [],
      "source": [
        "# select, similarly to a map, allows column values to be redefined\n",
        "df.select(df.population,df.median_house_value/1000).show()\n",
        "# put the operation within parenthesis and add .alias('median_house_value_in_K$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4whu8BylWfAK"
      },
      "outputs": [],
      "source": [
        "# withColumn: used to manipulate (rename, change the value, convert the datatype)\n",
        "# an existing column in a dataframe (or to create a new column) while keeping the rest intact\n",
        "df.withColumn('median_house_value_in_K$',df.median_house_value/1000).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zrnJn3_Q0PD"
      },
      "outputs": [],
      "source": [
        "# filter: returns a new DataFrame containing only the elements in the parent DataFrame that satisfy the function inside filter (as in RDDs)\n",
        "# orderBY: orders the DataFrame by the selected column(s)\n",
        "df.filter(df.population > 1000).orderBy(df.population.asc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qv5_B90Raho"
      },
      "outputs": [],
      "source": [
        "# groupBy: returns a new DataFrame which is the result of an aggregation\n",
        "df.groupBy(df.ocean_proximity).agg({'median_house_value': 'avg', '*': 'count'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3b1rxzMN7Lm"
      },
      "outputs": [],
      "source": [
        "# withColumnRenamed: rename a column\n",
        "df.groupBy(df.ocean_proximity).agg({'*': 'count'}).withColumnRenamed(\"count(1)\", \"tot\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TaP-GatdTD7k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|ocean_proximity|         avg_price|\n",
            "+---------------+------------------+\n",
            "|         ISLAND|          380440.0|\n",
            "|       NEAR BAY|259212.31179039303|\n",
            "|     NEAR OCEAN|249433.97742663656|\n",
            "|      <1H OCEAN|240084.28546409807|\n",
            "|         INLAND|124805.39200122119|\n",
            "+---------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL queries can be run on a DataFrame\n",
        "df.createOrReplaceTempView(\"housing\")\n",
        "spark.sql(\"select ocean_proximity, avg(median_house_value) as avg_price from housing group by ocean_proximity order by avg_price desc\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv5BUnaAWWEi"
      },
      "source": [
        "# Exercise: from Big Data to Smaller Data\n",
        "\n",
        "Case study: weather measurements from the National Climatic Data Center of the USA\n",
        "\n",
        "We are provided with the following dataset.\n",
        "\n",
        "- weather-stations.csv: it contains a list of weather stations that capture weather information every day of every year throughout the world\n",
        "  - Each station is identified by a StationID\n",
        "  - Additional information about the stations are available (coordinates, country, elevation, etc.)\n",
        "  - Not that big: ~30K stations, less than 3MB\n",
        "- weather-measurements.txt: it contains the data measured by a certain station on a certain date\n",
        "  - Each weather measurement is identified by a StationID and a Date\n",
        "  - Measurements provide a lot of information (about temperature, humidity, etc.)\n",
        "  - Can be pretty big! 30K stations * 365 days * 10 kinds of measurements = ~13GB per year\n",
        "  - Our file is a simplified -and smaller- version based on the real one\n",
        "\n",
        "Here, we want to exploit a big data tool (Spark) to make the data less expensive to store and still interesting for analytical purposes. In particular, we want to:\n",
        "- Clean the dataset\n",
        "- Join the tables\n",
        "- Produce a small(er) data cube that can be easily accessed by OLAP tools\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "The pipeline can be summarized as follows.\n",
        "\n",
        "1. Cleaning\n",
        "    1. On the stations file:\n",
        "        1. replace empty states and countries in stations with a placeholder value (e.g., \"XX\");\n",
        "        1. keep only the following fields: stationId, state, country\n",
        "    1. On the weather-sample file:\n",
        "        1. filter out weather wrong measurements (i.e., where airTemperatureQuality=9);\n",
        "        1. keep only the following fields: stationId, airTemperature, date, month, year\n",
        "        1. create a new fulldate field by concatenating year, month, and date in the format \"YYYY-MM-DD\"\n",
        "        1. create a new fullmonth field by concatenating year and month in the format \"YYYY-MM\"\n",
        "1. Integrating\n",
        "    1. Join stations with weather measurements on the stationId field\n",
        "    1. Keep only the following fields: state, country, fulldate, fullmonth, year, airTemperature\n",
        "1. Summarizing\n",
        "    1. Aggregate the measurements by state and date to take the average temperature\n",
        "        - Group by: state, country, fulldate, fullmonth, year\n",
        "        - Calculation: avg(airTemperature)\n",
        "1. Save the result on a file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO6bQ3NQwDAj"
      },
      "source": [
        "## Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcd4fAhgeDhH"
      },
      "outputs": [],
      "source": [
        "dfW = sc.textFile(\"../datasets/weather-measurements.txt\")\\\n",
        "  .map(lambda l: (\n",
        "    l[5:16],              # stationId\n",
        "    l[16:20],             # year\n",
        "    l[20:22],             # month\n",
        "    l[22:24],             # day\n",
        "    int(l[30:35]) / 10,   # airTemperature\n",
        "    l[35:36]              # airTemperatureQuality\n",
        "  ))\\\n",
        "  .toDF([\"stationId\",\"year\",\"month\",\"day\",\"airTemperature\",\"airTemperatureQuality\"])\n",
        "dfW.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaknyiXHeCbF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import concat\n",
        "dfS = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").csv(\"../datasets/weather-stations.csv\")\n",
        "dfS = dfS.select(concat(dfS[0],dfS[1]),dfS[2],dfS[3],dfS[4],dfS[5],dfS[6],dfS[7],dfS[8],dfS[9],dfS[10])\\\n",
        "  .toDF(\"stationId\",\"city\",\"country\",\"state\",\"call\",\"latitude\",\"longitude\",\"elevation\",\"date_begin\",\"date_end\")\n",
        "dfS.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqJZrglm-ZD"
      },
      "source": [
        "1. Cleaning\n",
        "    1. On the stations file:\n",
        "        1. replace empty states and countries in stations with a placeholder value (e.g., \"XX\");\n",
        "        1. keep only the following fields: stationId, state, country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v1YuN-4nQCQ"
      },
      "outputs": [],
      "source": [
        "dfS1 = dfS.fillna({'state': 'XX', 'country':'XX'})\n",
        "dfS2 = dfS1.select('stationId','state','country')\n",
        "dfS2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LxL1WQvnF0J"
      },
      "source": [
        "1. Cleaning\n",
        "    1. ...\n",
        "    1. On the weather-sample file:\n",
        "        1. filter out weather wrong measurements (i.e., where airTemperatureQuality=9);\n",
        "        1. keep only the following fields: stationId, airTemperature, date, month, year\n",
        "        1. create a new fulldate field by concatenating year, month, and date in the format \"YYYY-MM-DD\"\n",
        "        1. create a new fullmonth field by concatenating year and month in the format \"YYYY-MM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf9bm6CspR8x"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import concat, lit\n",
        "dfW1 = dfW.where(\"airTemperatureQuality < 9\")\n",
        "dfW2 = dfW1.select('stationId','airTemperature','day','month','year')\n",
        "dfW3 = dfW2.withColumn(\"fulldate\", concat(dfW1.year,lit(\"-\"),dfW1.month,lit(\"-\"),dfW1.day))\n",
        "dfW4 = dfW3.withColumn(\"fullmonth\", concat(dfW1.year,lit(\"-\"),dfW1.month))\n",
        "dfW4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy2yp35unHhO"
      },
      "source": [
        "2. Integrating\n",
        "    1. Join stations with weather measurements on the stationId field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHTKQhQJrPMU"
      },
      "outputs": [],
      "source": [
        "dfJ = dfS2.join(dfW4, \"stationId\")\n",
        "dfJ.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv_dfniznJ2A"
      },
      "source": [
        "2. Integrating\n",
        "    1. ...\n",
        "    1. Keep only the following fields: state, country, fulldate, fullmonth, year, airTemperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYyMxZaSrprK"
      },
      "outputs": [],
      "source": [
        "dfJ2 = dfJ.select(\"state\", \"country\", \"fulldate\", \"fullmonth\", \"year\", \"airTemperature\")\n",
        "dfJ2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXKChhXwnLlv"
      },
      "source": [
        "3. Summarizing\n",
        "    1. Aggregate the measurements by state and date to take the average temperature\n",
        "        - Group by: state, country, fulldate, fullmonth, year\n",
        "        - Calculation: avg(airTemperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shVr9k9hryqH"
      },
      "outputs": [],
      "source": [
        "dfG = dfJ2.groupBy(\"state\", \"country\", \"fulldate\", \"fullmonth\", \"year\").agg({'airTemperature': 'avg'})\n",
        "dfG.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwXx8h-tv4-v"
      },
      "source": [
        "4. Save the result on a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pKTH93LQu61U"
      },
      "outputs": [],
      "source": [
        "dfG.write.mode('overwrite').option('header','true').csv(\"./weather-cube\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkKWX-mWv8S9"
      },
      "source": [
        "## Analytics\n",
        "\n",
        "Load the data from the CSV file and use the technique of your preference to:\n",
        "- Show the average temperature by country on a map\n",
        "- Visualize the daily trend of average temperatures for Italy, France, Switzerland, Norway, and USA\n",
        "\n",
        "Feel free to ask this to an LLM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "679cmN5vLOMH"
      },
      "source": [
        "# Additional exercises\n",
        "\n",
        "### Getting familiar with data frame transformations\n",
        "\n",
        "Carry out the following operations (in any order).\n",
        "\n",
        "- ```.select()``` operator; starting from ```dfS```:\n",
        "  1. keep only country, elevation, date_begin, and date_end\n",
        "  1. keep only the first four characters of date_begin using [sf.substring](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substring.html)\n",
        "  1. use [concat](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html) to concatenate date_begin with date_end and putting an underscore (\\_) in the middle; since the underscore is not a column, declare it as ```sf.lit(\"_\")```\n",
        "  1. use [coalesce](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html) to take the value of country if not null, otherwise the value of city\n",
        "  1. put countries in lowercase using [sf.lower](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lower.html)\n",
        "  1. in the previous four points, use ```.alias()``` to give a meaningful name to the obtained columns\n",
        "- ```.withColumn()``` operator; starting from ```dfS```:\n",
        "  1. do the same as points 2 to 5 of the ```select``` operator, but using ```withColumn```\n",
        "- ```.filter()``` operator; starting from ```dfS```, keep only the rows where:\n",
        "  1. elevation is greater than 5000\n",
        "  1. country is not null, using [sf.isnotnull](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnotnull.html)\n",
        "  1. conditions 1 and 2 are both true; conditions must be put between parenthesis and separated by \"&\" (e.g., check [here](https://www.geeksforgeeks.org/pyspark-filter-dataframe-based-on-multiple-conditions/))\n",
        "  1. either one of conditions 1 and 2 is true; conditions must be put between parenthesis and separated by \"|\" (e.g., check [here](https://www.geeksforgeeks.org/pyspark-filter-dataframe-based-on-multiple-conditions/))\n",
        "  1. date_begin is the first day of the month (requires to use substring)\n",
        "- ```.groupBy()``` operator; starting from ```dfW```:\n",
        "  1. group by airTemperatureQuality to count how many rows there are for each value\n",
        "  1. as above, but also calculate the average temperature\n",
        "  1. as above, but also given meaningful names to the results using ```withColumnRenamed```\n",
        "  1. group by month to calculate the minimum and maximum temperatures and order by month using ```orderby```; to aggregate differently on the same column, use the [sf.max](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.max.html) and [sf.min](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.min.html) functions inside the ```agg``` function instead of the object enclosed by brackets ```{}```\n",
        "  1. group by month and day to calculate the minimum and maximum temperatures\n",
        "  1. group by stationId and year month to count the number of rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFGe5AvcHCKM"
      },
      "source": [
        "### Complete exercises\n",
        "\n",
        "Carry out the following exercises (in any order).\n",
        "\n",
        "- Check if there exist stations with a negative elevation; then, calculate how many of these stations exist in each country; rename the result to \"cnt\" and order the result by decreasing cnt\n",
        "- Take only stations with positive elevation, compute the maximum elevation by country and rename the result to \"elevation\"; then, join the result with the original dfS to get, for each country, the name of the city with the highest elevation (join key: ```[\"country\",\"elevation\"]```); order the result by decreasing elevation\n",
        "- Take only weather values with airQuality==1, compute the minimum temperature for each stationId and rename it to \"minTemperature\"; then, join the result with dfS and keep only the columns \"minTemperature\" and \"elevation\"; finally, use the correlation between the two columns. To do the last part, you need to:\n",
        "  - cast the elevation to an integer datatype: you need to add ```from pyspark.sql.types import IntegerType``` and then ```df.myfield.cast(IntegerType())```;\n",
        "  - compute the correlation with ```df.stat.corr(\"myfield1\",\"myfield2\")```."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
